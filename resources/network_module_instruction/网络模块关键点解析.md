# 1. 网络模块关键点

网络模块为服务端/客户端一体(P2P对等)，服务端和客户端共享已连接节点信息（nodeId等），假设A节点已经与B节点建立连接，B再连接A时，A发现本节点已经存在与B的连接，会拒绝B发起的连接，可以达到A节点与B节点互联共享一条连接的目的。

## 1.1 服务端关键点

1. 采用reactor主从模式编写，epoll边缘触发模式管理fd。
2. 主reactor中只需要监测监听套接字是否有连接上来，感觉监听套接字设置为非阻塞没太大意义，故采用select模型(Selector)检测监听套接字是否有连接上线。
3. 检测到有连接上线后，通知Acceptor，Acceptor有自己的独立线程，负责accept到客户端的连接套接字，然后创建会话(TcpSession)，客户端的连接套接字（文档后续部分介绍直接使用fd或者TcpSession指代某个客户端）会包含在TcpSession中，最后将TcpSession回调到TcpService写作类中，在TcpService中由TcpSessionDispatcher配合决定TcpSession分发到哪一个SlaveReactor中。
4. 关于SlaveReactorManager的负载均衡策略是每派发50个TcpSession到SlaveReactor后，检测一次哪个SlaveReactor管理的TcpSession最少（假设这里管理TcpSession最少的SlaveReactor为minTcpSessionSlaveReactor），那么接下来的50个TcpSession将会分配到minTcpSessionSlaveReactor，又因为随时都可能有客户端离线，所以各个SlaveReactor管理的TcpSession并不是相同的，也不是严格相差50个。
5. 用来收发客户端数据的环形缓冲区包含在TcpSession中，环形缓冲区的实现是libcomponents中的RingBuffer.h/.cpp。
6. 每次EPOLLIN事件到来，则收取Socket内核缓冲区大小的数据，保证数据一次收完，保证边缘触发最佳效率。
7. 将收到的数据抛给SessionDataProcessor中的队列，由工作线程池进行解包，回调给业务进行处理。
8. 回调函数根据包里面的moduleId将包派发给各个模组进行处理。

##### 心跳包当数据量非常大的时候处理被阻塞问题解决

- SessionDataProcessor为接收到的数据处理类，原来是由一个工作线程池负责（解包头，包体，回调包给业务层进行处理），增加了一个专门处理心跳的线程。

## 1.2 网络客户端关键点解析

1. 客户端连接上服务端后，同样创建一个TcpSession代表对端，TcpSession也是回调到TcpService并配合TcpSessionDispatcher决定派发到哪一个SlaveReactor。

2. 客户端有两个主要类：HostsConnector，HostsInfoManager。HostsInfosConnector不存储节点信息，向HostsInfoManager获取节点信息，使用非阻塞套接字及Select模型进行异步连接。

3. HostsInfoManager有两个主要数据结构：

   ```c++
   std::unordered_map<HostEndPointInfo, std::pair<std::string, std::uint64_t>, HostEndPointInfo::hashFunction> m_hosts;
   ```

   HostsEndPointInfo是对端节点的ip, port二元组信息，当连接上对端节点的时候，将对端节点的nodeId信息填充到m_hosts[hostEndPointInfo].first中，HostsConnector会认为对端节点已经连接上，不会再尝试连接此节点。对端节点离线时，m_hosts[hostEndPointInfo].first被清空，HostsConnector再次尝试连接。

   ```c++
   // NodeId => (fd, handshakeUuid)
   std::unordered_map<std::string, std::pair<int, std::string>> m_nodeIdInfos;
   ```

该数据结构存储节点NodeId和对应到该节点的fd，用来发送数据时根据nodeId找到fd，至于handshakeUuid，请参考文档2（节点ID互相连接共享一条连接的协议文档）

## UML Class

![](./network_uml_class.jpg)